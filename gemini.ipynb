{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2024 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/google-deepmind/recurrentgemma.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch kagglehub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import json\n",
    "# Replace 'path/to/kaggle.json' with your actual file path\n",
    "config = {\"username\":\"huzaifaghori\",\"key\":\"c08f2b54aa777c1bd232e5cfc7b66ea8\"}\n",
    "\n",
    "api = kaggle.api\n",
    "# Now you can use the Kaggle API with 'api' object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "api._load_config(config_data=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import pathlib\n",
    "import kagglehub\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from recurrentgemma import torch as recurrentgemma\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT = '2b-it' # @param ['2b', '2b-it'] {type:\"string\"}\n",
    "weights_dir = kagglehub.model_download(f'google/recurrentgemma/PyTorch/{VARIANT}')\n",
    "# weights_dir = pathlib.Path(f\"/input/recurrentgemma/pytorch/{VARIANT}/1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_path = f\"{weights_dir}/{VARIANT}.pt\"\n",
    "vocab_path = f\"{weights_dir}/tokenizer.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "params = torch.load(str(ckpt_path))\n",
    "params = {k : v.to(device=device) for k, v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = recurrentgemma.GriffinConfig.from_torch_params(params)\n",
    "model = recurrentgemma.Griffin(model_config, device=device, dtype=torch.bfloat16)\n",
    "model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(str(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = recurrentgemma.Sampler(model=model, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is the president of USA?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m ]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 300 generation steps\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m out_data \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_generation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_data\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_string, out_string \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_batch, out_data\u001b[38;5;241m.\u001b[39mtext):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recurrentgemma\\torch\\sampler.py:286\u001b[0m, in \u001b[0;36mSampler.__call__\u001b[1;34m(self, input_strings, total_generation_steps, echo, return_logits)\u001b[0m\n\u001b[0;32m    276\u001b[0m next_token \u001b[38;5;241m=\u001b[39m next_token[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    278\u001b[0m initial_sampling_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_sample_state(\n\u001b[0;32m    279\u001b[0m     positions\u001b[38;5;241m=\u001b[39mpositions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    280\u001b[0m     last_token\u001b[38;5;241m=\u001b[39mnext_token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     include_logits\u001b[38;5;241m=\u001b[39mreturn_logits,\n\u001b[0;32m    284\u001b[0m )\n\u001b[1;32m--> 286\u001b[0m sampling_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_sampling_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m token_buffer \u001b[38;5;241m=\u001b[39m sampling_state\u001b[38;5;241m.\u001b[39mtoken_buffer[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    289\u001b[0m logits_buffer \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    290\u001b[0m     sampling_state\u001b[38;5;241m.\u001b[39mlogits_buffer[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;28;01mif\u001b[39;00m return_logits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    291\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recurrentgemma\\torch\\sampler.py:205\u001b[0m, in \u001b[0;36mSampler._sample_fn\u001b[1;34m(self, initial_sampling_state)\u001b[0m\n\u001b[0;32m    200\u001b[0m sampler_state \u001b[38;5;241m=\u001b[39m initial_sampling_state\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[0;32m    202\u001b[0m     (sampler_state\u001b[38;5;241m.\u001b[39mdecoding_step \u001b[38;5;241m<\u001b[39m sampler_state\u001b[38;5;241m.\u001b[39mtotal_sampling_steps) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m    203\u001b[0m     torch\u001b[38;5;241m.\u001b[39many(torch\u001b[38;5;241m.\u001b[39mlogical_not(sampler_state\u001b[38;5;241m.\u001b[39mdone))\n\u001b[0;32m    204\u001b[0m ):\n\u001b[1;32m--> 205\u001b[0m   sampler_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampler_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sampler_state\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recurrentgemma\\torch\\sampler.py:104\u001b[0m, in \u001b[0;36mSampler._sample_step\u001b[1;34m(self, sampler_state)\u001b[0m\n\u001b[0;32m    101\u001b[0m last_token \u001b[38;5;241m=\u001b[39m sampler_state\u001b[38;5;241m.\u001b[39mtoken_buffer[:, decoding_step]\n\u001b[0;32m    102\u001b[0m last_token \u001b[38;5;241m=\u001b[39m last_token[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 104\u001b[0m logits, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoding_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m next_token_candidate \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 1]\u001b[39;00m\n\u001b[0;32m    111\u001b[0m next_token_candidate \u001b[38;5;241m=\u001b[39m next_token_candidate[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# [B,]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recurrentgemma\\torch\\griffin.py:128\u001b[0m, in \u001b[0;36mGriffin.forward\u001b[1;34m(self, tokens, segment_pos, cache)\u001b[0m\n\u001b[0;32m    126\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlogits_soft_cap\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m c\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, new_cache\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:1965\u001b[0m, in \u001b[0;36mtanh\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m   1953\u001b[0m softshrink \u001b[38;5;241m=\u001b[39m _add_docstr(\n\u001b[0;32m   1954\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msoftshrink,\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m,\n\u001b[0;32m   1962\u001b[0m )\n\u001b[1;32m-> 1965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtanh\u001b[39m(\u001b[38;5;28minput\u001b[39m):  \u001b[38;5;66;03m# noqa: D400,D402\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"tanh(input) -> Tensor\u001b[39;00m\n\u001b[0;32m   1967\u001b[0m \n\u001b[0;32m   1968\u001b[0m \u001b[38;5;124;03m    Applies element-wise,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.Tanh` for more details.\u001b[39;00m\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mtanh()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_batch = [\n",
    "  \"who is the president of USA?\",\n",
    "    \"?\"\n",
    "]\n",
    "\n",
    "# 300 generation steps\n",
    "out_data = sampler(input_strings=input_batch, total_generation_steps=300)\n",
    "print(out_data.text)\n",
    "for input_string, out_string in zip(input_batch, out_data.text):\n",
    "  print(f\"Prompt:\\n{input_string}\\nOutput:\\n{out_string}\")\n",
    "  print(10*'#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
